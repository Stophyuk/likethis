import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'

// Vercel í•¨ìˆ˜ íƒ€ì„ì•„ì›ƒ ì„¤ì • (ìµœëŒ€ 60ì´ˆ)
export const maxDuration = 60

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
})

interface ChatMessage {
  date: string
  user: string
  message: string
}

interface Insight {
  category: 'command' | 'number' | 'solution' | 'tool' | 'trend' | 'business'
  title: string
  content: string
  tags: string[]
  sourceQuotes?: string[]  // ì›ë³¸ ëŒ€í™” ì¸ìš©
}

// AI ì¸ì‚¬ì´íŠ¸ í™œìš© ì œì•ˆ
interface ActionableInsight {
  type: 'blog' | 'project' | 'learning' | 'networking'
  title: string
  description: string
  relatedInsightTitles: string[]
}

interface ChunkInsights {
  insights: Insight[]
  resources: Array<{ url: string; description: string }>
  keyTopics: string[]
}

// Generate actionable insights from extracted insights
async function generateActionableInsights(insights: Insight[]): Promise<ActionableInsight[]> {
  if (insights.length === 0) return []

  const insightSummary = insights.map((i, idx) =>
    `${idx + 1}. [${i.category}] ${i.title}: ${i.content}`
  ).join('\n')

  const prompt = `ì¶”ì¶œëœ ì¸ì‚¬ì´íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆì„ ìƒì„±í•˜ì„¸ìš”.

## ì¸ì‚¬ì´íŠ¸ ëª©ë¡
${insightSummary}

## 4ê°€ì§€ í™œìš© ìœ í˜•
1. **blog**: ë¸”ë¡œê·¸/SNS ê¸€ê° (ê¸°ìˆ  ë¸”ë¡œê·¸, ë§í¬ë“œì¸ í¬ìŠ¤íŠ¸)
2. **project**: ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´
3. **learning**: í•™ìŠµ í¬ì¸íŠ¸ (ê³µë¶€í•  ê²ƒ, ìŠ¤í‚¬ ì—…)
4. **networking**: ë„¤íŠ¸ì›Œí‚¹ ê¸°íšŒ (ì»¤ë®¤ë‹ˆí‹° í™œë™, ë°‹ì—…)

## ê·œì¹™
- ê° ìœ í˜•ë³„ë¡œ 1-2ê°œì”© ì œì•ˆ (ì´ 4-8ê°œ)
- ì‹¤ì œë¡œ ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  êµ¬ì²´ì ì¸ ì œì•ˆ
- ì¸ì‚¬ì´íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê·¼ê±° ìˆëŠ” ì œì•ˆ
- relatedInsightTitlesì— ê´€ë ¨ ì¸ì‚¬ì´íŠ¸ ì œëª© í¬í•¨

JSON:
{
  "actionable": [
    {
      "type": "blog|project|learning|networking",
      "title": "ì œëª©",
      "description": "êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ë²•",
      "relatedInsightTitles": ["ê´€ë ¨ ì¸ì‚¬ì´íŠ¸ ì œëª©"]
    }
  ]
}`

  try {
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
      max_tokens: 2000,
      temperature: 0.5,
    })

    const result = JSON.parse(completion.choices[0].message.content || '{"actionable":[]}')
    return result.actionable || []
  } catch (error) {
    console.error('Failed to generate actionable insights:', error)
    return []
  }
}

// ì²­í¬ë³„ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
async function extractChunkInsights(messages: ChatMessage[], chunkIndex: number, totalChunks: number): Promise<ChunkInsights> {
  const chatContent = messages
    .map(m => m.message)
    .join('\n')
    .substring(0, 20000)

  const prompt = `IT/ê°œë°œì ì»¤ë®¤ë‹ˆí‹° ëŒ€í™”ì—ì„œ **ìœ ìš©í•œ ì§€ì‹ê³¼ íŠ¸ë Œë“œ**ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
(ì²­í¬ ${chunkIndex + 1}/${totalChunks})

## ğŸš« ê¸ˆì§€ í‘œí˜„
- "~ì— ëŒ€í•´ ë…¼ì˜/ì´ì•¼ê¸°/ì–¸ê¸‰ë˜ì—ˆë‹¤"
- "~ì— ëŒ€í•œ ì •ë³´ê°€ ê³µìœ ë˜ì—ˆë‹¤"
- "ë‹¤ì–‘í•œ ~ê°€ ë‹¤ë£¨ì–´ì¡Œë‹¤"

## ğŸ“Œ ì¶”ì¶œ ëŒ€ìƒ (6ê°€ì§€ ì¹´í…Œê³ ë¦¬)

### 1. command - ëª…ë ¹ì–´/ì„¤ì •
ì˜ˆ: "/compactë¡œ ì»¨í…ìŠ¤íŠ¸ ì••ì¶•", "user-invocable: true ì„¤ì •"

### 2. number - ìˆ˜ì¹˜/ê°€ê²©
ì˜ˆ: "Claude Max $100/ì›”", "RAM ìµœì†Œ 24GB", "75% í•´ê³ "

### 3. solution - ë¬¸ì œâ†’í•´ê²°
ì˜ˆ: "í™”ë©´ ê¹¨ì§ â†’ ì „ì²´í™”ë©´ ì „í™˜ìœ¼ë¡œ í•´ê²°"

### 4. tool - ë„êµ¬ ì¶”ì²œ/ë¹„êµ
ì˜ˆ: "ghostty í„°ë¯¸ë„ ì¶”ì²œ", "chrome devtools > playwright"

### 5. trend - ì‹œì¥/ê¸°ìˆ  íŠ¸ë Œë“œ â­
ì˜ˆ: "ì±„ìš©ì‹œì¥ ì–¼ì–´ë¶™ìŒ", "AI ê²½í—˜ ì—†ìœ¼ë©´ ì„œë¥˜ íƒˆë½", "ì˜¤í”ˆì½”ë“œê°€ í•«í•¨"

### 6. business - ë¹„ì¦ˆë‹ˆìŠ¤/ìˆ˜ìµ ì¸ì‚¬ì´íŠ¸ â­
ì˜ˆ: "ë¶€ì—…ìœ¼ë¡œ ì›”ê¸‰ë³´ë‹¤ ë” ë²Œê³  ìˆìŒ", "ì‡¼ì¸  ìë™í™”ê°€ ìˆ˜ìµ ë³´ì¥ ì•ˆí•¨"

## ğŸ“ ì˜ˆì‹œ

ì…ë ¥: "ìš”ì¦˜ ì±„ìš©ì‹œì¥ ì™„ì „ ì–¼ì—ˆë‹¤" "AI ì•ˆí–ˆë‹¤ê³  í•˜ë©´ íŒ¨ìŠ¤"
âœ… {"category":"trend","title":"ê°œë°œì ì±„ìš© ì‹œì¥","content":"í˜„ì¬ ê°œë°œì ì±„ìš©ì‹œì¥ì´ ì–¼ì–´ë¶™ì€ ìƒíƒœ. AI ê²½í—˜ì´ ì—†ìœ¼ë©´ ì„œë¥˜ ë‹¨ê³„ì—ì„œ íƒˆë½í•˜ëŠ” ê²½ìš°ê°€ ë§ìŒ.","tags":["ì±„ìš©","AI"],"sourceQuotes":["ìš”ì¦˜ ì±„ìš©ì‹œì¥ ì™„ì „ ì–¼ì—ˆë‹¤","AI ì•ˆí–ˆë‹¤ê³  í•˜ë©´ íŒ¨ìŠ¤"]}

ì…ë ¥: "ì›”ê¸‰ë³´ë‹¤ ë¶€ì—…ìœ¼ë¡œ ë” ë²Œê³  ìˆì–´ì„œ"
âœ… {"category":"business","title":"ë¶€ì—… ìˆ˜ìµ","content":"ë°”ì´ë¸Œì½”ë”©ìœ¼ë¡œ ë¶€ì—… ì‹œ ë³¸ì—… ì›”ê¸‰ë³´ë‹¤ ë” ë§ì´ ë²„ëŠ” ì‚¬ë¡€ê°€ ìˆìŒ.","tags":["ë¶€ì—…","ìˆ˜ìµ"],"sourceQuotes":["ì›”ê¸‰ë³´ë‹¤ ë¶€ì—…ìœ¼ë¡œ ë” ë²Œê³  ìˆì–´ì„œ"]}

âŒ ê¸ˆì§€: {"content":"AI ë„êµ¬ ì‚¬ìš© ê²½í—˜ì´ ê³µìœ ë˜ì—ˆë‹¤"}

## ì¶”ì¶œ ì›ì¹™
- êµ¬ì²´ì  ë„êµ¬ëª…, ìˆ˜ì¹˜, í˜„ìƒì„ í¬í•¨í•  ê²ƒ
- íŠ¸ë Œë“œ/ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë„ ì ê·¹ ì¶”ì¶œ
- **sourceQuotes í•„ìˆ˜**: ì¸ì‚¬ì´íŠ¸ ê·¼ê±°ê°€ ëœ ì›ë³¸ ëŒ€í™” 1-3ê°œ ì¸ìš©
- ë©”íƒ€ ì„¤ëª…("ë…¼ì˜ë˜ì—ˆë‹¤")ì€ ì ˆëŒ€ ê¸ˆì§€

JSON: {"insights":[{"category":"","title":"","content":"","tags":[],"sourceQuotes":["ì›ë³¸ì¸ìš©"]}],"resources":[{"url":"","description":""}],"keyTopics":[]}

ëŒ€í™”:
${chatContent}`

  const completion = await openai.chat.completions.create({
    model: 'gpt-4o-mini',
    messages: [{ role: 'user', content: prompt }],
    response_format: { type: 'json_object' },
    max_tokens: 4000,
    temperature: 0.3,
  })

  return JSON.parse(completion.choices[0].message.content || '{"insights":[],"resources":[],"keyTopics":[]}')
}

// ì¸ì‚¬ì´íŠ¸ ì¢…í•©
async function synthesizeInsights(
  chunkInsights: ChunkInsights[],
  totalMessageCount: number
): Promise<Record<string, unknown>> {
  const allInsights = chunkInsights.flatMap(c => c.insights)
  const allResources = chunkInsights.flatMap(c => c.resources || [])
  const allTopics = [...new Set(chunkInsights.flatMap(c => c.keyTopics || []))]

  // ì¤‘ë³µ ì œê±°
  const uniqueInsights: Insight[] = []
  const seenTitles = new Set<string>()

  for (const insight of allInsights) {
    const normalizedTitle = insight.title?.toLowerCase().trim()
    if (normalizedTitle && !seenTitles.has(normalizedTitle)) {
      seenTitles.add(normalizedTitle)
      uniqueInsights.push(insight)
    }
  }

  const uniqueResources = allResources.filter((r, i, arr) =>
    r.url && arr.findIndex(x => x.url === r.url) === i
  )

  if (uniqueInsights.length > 0) {
    const prompt = `IT/ê°œë°œì ì»¤ë®¤ë‹ˆí‹° ëŒ€í™”ì—ì„œ ì¶”ì¶œëœ ì§€ì‹ë“¤ì„ ì •ë¦¬í•˜ì„¸ìš”.
ì´ ${totalMessageCount.toLocaleString()}ê°œ ë©”ì‹œì§€, ${uniqueInsights.length}ê°œ ì¸ì‚¬ì´íŠ¸.

## ì¶”ì¶œëœ ì§€ì‹
${uniqueInsights.map((i, idx) => `${idx + 1}. [${i.category}] ${i.title}: ${i.content}${i.sourceQuotes ? ` (ì›ë¬¸: "${i.sourceQuotes.join('", "')}")` : ''}`).join('\n')}

## ê³µìœ ëœ ìë£Œ
${uniqueResources.map(r => `- ${r.url}: ${r.description}`).join('\n')}

## ğŸš« ê¸ˆì§€ í‘œí˜„
"~ì— ëŒ€í•´ ë…¼ì˜/ì´ì•¼ê¸°ë˜ì—ˆë‹¤", "ì •ë³´ê°€ ê³µìœ ë˜ì—ˆë‹¤", "ë‹¤ì–‘í•œ ~ê°€ ë‹¤ë£¨ì–´ì¡Œë‹¤"

## âœ… ì‘ì„± ê·œì¹™
1. **insights**: ì¤‘ë³µ ë³‘í•©, ì¹´í…Œê³ ë¦¬ë³„ ì •ë¦¬
   - command/number/solution/tool: êµ¬ì²´ì  ê°’ í¬í•¨
   - trend/business: ì‹œì¥ íŠ¸ë Œë“œ, ìˆ˜ìµ ì¸ì‚¬ì´íŠ¸
   - **sourceQuotes í•„ìˆ˜**: ìœ„ì— ì œê³µëœ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ í¬í•¨
2. **summary**: í•µì‹¬ ë°œê²¬ 5-7ê°œë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ì—´
   - âŒ "ì±„ìš©ì‹œì¥ì— ëŒ€í•´ ë…¼ì˜ë˜ì—ˆë‹¤"
   - âœ… "ì±„ìš©ì‹œì¥ì´ ì–¼ì–´ë¶™ìŒ, AI ê²½í—˜ ì—†ìœ¼ë©´ ì„œë¥˜ íƒˆë½ ë§ìŒ"
3. **highlights**: ê°€ì¥ ì¸ìƒì ì¸ ì¸ì‚¬ì´íŠ¸ 3ê°œ
   - íŠ¸ë Œë“œ, ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ìš°ì„ 

JSON:
{
  "insights": [{"category":"command|number|solution|tool|trend|business","title":"","content":"","tags":[],"sourceQuotes":["ì›ë³¸ì¸ìš©"]}],
  "resources": [{"url":"","title":"","description":""}],
  "summary": "í•µì‹¬ ë°œê²¬ ë‚˜ì—´ì‹ (ë…¼ì˜ë˜ì—ˆë‹¤ ê¸ˆì§€)",
  "highlights": ["ì¸ìƒì ì¸ ì¸ì‚¬ì´íŠ¸ 3ê°œ"],
  "topKeywords": ["í‚¤ì›Œë“œ 10ê°œ"]
}`

    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
      max_tokens: 6000,
      temperature: 0.3,
    })

    const result = JSON.parse(completion.choices[0].message.content || '{}')
    return {
      ...result,
      _meta: {
        totalMessages: totalMessageCount,
        rawInsightCount: allInsights.length,
        uniqueInsightCount: uniqueInsights.length,
        model: 'gpt-4o-mini'
      }
    }
  }

  return {
    insights: [],
    summary: 'ì˜ë¯¸ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.',
    resources: uniqueResources,
    noInsights: true,
  }
}

export async function POST(req: NextRequest) {
  try {
    const { chatContent, roomName, messages } = await req.json()

    let chatMessages: ChatMessage[] = []

    if (messages && Array.isArray(messages)) {
      chatMessages = messages
    } else if (chatContent) {
      const lines = chatContent.split('\n').filter((l: string) => l.trim())
      chatMessages = lines.map((line: string) => {
        const match = line.match(/\[([^\]]+)\]\s*([^:]+):\s*(.+)/)
        if (match) {
          return { date: match[1], user: match[2], message: match[3] }
        }
        return { date: '', user: '', message: line }
      })
    }

    if (chatMessages.length === 0) {
      return NextResponse.json({ error: 'Chat content is required' }, { status: 400 })
    }

    const totalCount = chatMessages.length
    console.log(`[OpenAI] Analyzing ${totalCount} messages for room: ${roomName}`)

    if (totalCount <= 500) {
      // 500ê°œ ì´í•˜: ë‹¨ì¼ ë¶„ì„
      const chatText = chatMessages.map(m => m.message).join('\n')

      const prompt = `IT/ê°œë°œì ì»¤ë®¤ë‹ˆí‹° ëŒ€í™”ì—ì„œ **ìœ ìš©í•œ ì§€ì‹ê³¼ íŠ¸ë Œë“œ**ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

## ğŸš« ê¸ˆì§€ í‘œí˜„
"~ì— ëŒ€í•´ ë…¼ì˜/ì´ì•¼ê¸°ë˜ì—ˆë‹¤", "ì •ë³´ê°€ ê³µìœ ë˜ì—ˆë‹¤", "ë‹¤ì–‘í•œ ~ê°€ ë‹¤ë£¨ì–´ì¡Œë‹¤"

## ğŸ“Œ 6ê°€ì§€ ì¹´í…Œê³ ë¦¬
1. **command**: ëª…ë ¹ì–´, ì„¤ì •ê°’ (ì˜ˆ: /compact, user-invocable: true)
2. **number**: ê°€ê²©, ìˆ˜ì¹˜ (ì˜ˆ: $100/ì›”, 24GB, 75% í•´ê³ )
3. **solution**: ë¬¸ì œâ†’í•´ê²° (ì˜ˆ: í™”ë©´ ê¹¨ì§ â†’ ì „ì²´í™”ë©´ìœ¼ë¡œ í•´ê²°)
4. **tool**: ë„êµ¬ ì¶”ì²œ/ë¹„êµ (ì˜ˆ: ghostty ì¶”ì²œ, chrome devtools > playwright)
5. **trend**: ì‹œì¥/ê¸°ìˆ  íŠ¸ë Œë“œ â­ (ì˜ˆ: ì±„ìš©ì‹œì¥ ì–¼ì–´ë¶™ìŒ, AI ê²½í—˜ í•„ìˆ˜)
6. **business**: ë¹„ì¦ˆë‹ˆìŠ¤/ìˆ˜ìµ â­ (ì˜ˆ: ë¶€ì—…ìœ¼ë¡œ ì›”ê¸‰ë³´ë‹¤ ë” ë²Œê³  ìˆìŒ)

## ğŸ“ ì˜ˆì‹œ
"ì±„ìš©ì‹œì¥ ì™„ì „ ì–¼ì—ˆë‹¤" "AI ì•ˆí–ˆë‹¤ê³  í•˜ë©´ íŒ¨ìŠ¤"
âœ… {"category":"trend","title":"ì±„ìš© ì‹œì¥","content":"ê°œë°œì ì±„ìš©ì‹œì¥ ì–¼ì–´ë¶™ìŒ. AI ê²½í—˜ ì—†ìœ¼ë©´ ì„œë¥˜ íƒˆë½ ë§ìŒ","tags":["ì±„ìš©","AI"],"sourceQuotes":["ì±„ìš©ì‹œì¥ ì™„ì „ ì–¼ì—ˆë‹¤","AI ì•ˆí–ˆë‹¤ê³  í•˜ë©´ íŒ¨ìŠ¤"]}

"ì›”ê¸‰ë³´ë‹¤ ë¶€ì—…ìœ¼ë¡œ ë” ë²Œê³  ìˆì–´ì„œ"
âœ… {"category":"business","title":"ë¶€ì—… ìˆ˜ìµ","content":"ë°”ì´ë¸Œì½”ë”© ë¶€ì—…ìœ¼ë¡œ ë³¸ì—… ì›”ê¸‰ë³´ë‹¤ ë” ë²„ëŠ” ì‚¬ë¡€ ìˆìŒ","tags":["ë¶€ì—…","ìˆ˜ìµ"],"sourceQuotes":["ì›”ê¸‰ë³´ë‹¤ ë¶€ì—…ìœ¼ë¡œ ë” ë²Œê³  ìˆì–´ì„œ"]}

âŒ ê¸ˆì§€: {"content":"ë„êµ¬ ì‚¬ìš© ê²½í—˜ì´ ê³µìœ ë˜ì—ˆë‹¤"}

## ì¶”ì¶œ ì›ì¹™
- **sourceQuotes í•„ìˆ˜**: ì¸ì‚¬ì´íŠ¸ ê·¼ê±°ê°€ ëœ ì›ë³¸ ëŒ€í™” 1-3ê°œ ì¸ìš©

## JSON
{
  "insights": [{"category":"","title":"","content":"","tags":[],"sourceQuotes":["ì›ë³¸ì¸ìš©"]}],
  "resources": [{"url":"","title":"","description":""}],
  "summary": "í•µì‹¬ ë°œê²¬ ë‚˜ì—´ (ë…¼ì˜ë˜ì—ˆë‹¤ ê¸ˆì§€)",
  "highlights": ["ì¸ìƒì ì¸ ì¸ì‚¬ì´íŠ¸ 3ê°œ"],
  "topKeywords": ["10ê°œ"]
}

ëŒ€í™”:
${chatText.substring(0, 25000)}`

      const completion = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [{ role: 'user', content: prompt }],
        response_format: { type: 'json_object' },
        max_tokens: 6000,
        temperature: 0.3,
      })

      const result = JSON.parse(completion.choices[0].message.content || '{}')

      // Generate actionable insights
      const actionable = await generateActionableInsights(result.insights || [])

      return NextResponse.json({
        ...result,
        actionable,
        _meta: {
          totalMessages: totalCount,
          analysisMethod: 'single',
          model: 'gpt-4o-mini'
        }
      })

    } else {
      // 500ê°œ ì´ˆê³¼: ì²­í¬ ë¶„ì„
      const CHUNK_SIZE = 400
      const chunks: ChatMessage[][] = []

      for (let i = 0; i < chatMessages.length; i += CHUNK_SIZE) {
        chunks.push(chatMessages.slice(i, i + CHUNK_SIZE))
      }

      console.log(`[OpenAI] Processing ${chunks.length} chunks...`)

      // ìµœëŒ€ 8ê°œ ì²­í¬ ê· ë“± ìƒ˜í”Œë§
      let chunksToAnalyze: ChatMessage[][]
      if (chunks.length <= 8) {
        chunksToAnalyze = chunks
      } else {
        const step = Math.floor(chunks.length / 8)
        chunksToAnalyze = []
        for (let i = 0; i < 8; i++) {
          chunksToAnalyze.push(chunks[Math.min(i * step, chunks.length - 1)])
        }
      }

      console.log(`[OpenAI] Analyzing ${chunksToAnalyze.length} chunks out of ${chunks.length}...`)

      // ìˆœì°¨ ì²˜ë¦¬ (rate limit ë°©ì§€)
      const chunkInsights: ChunkInsights[] = []
      for (let i = 0; i < chunksToAnalyze.length; i++) {
        console.log(`[OpenAI] Processing chunk ${i + 1}/${chunksToAnalyze.length}...`)
        const result = await extractChunkInsights(chunksToAnalyze[i], i, chunksToAnalyze.length)
        chunkInsights.push(result)

        if (i < chunksToAnalyze.length - 1) {
          await new Promise(resolve => setTimeout(resolve, 500))
        }
      }

      const result = await synthesizeInsights(chunkInsights, totalCount)

      // Generate actionable insights
      const actionable = await generateActionableInsights((result.insights as Insight[]) || [])

      return NextResponse.json({
        ...result,
        actionable,
        _meta: {
          ...((result._meta as object) || {}),
          totalChunks: chunks.length,
          chunksAnalyzed: chunksToAnalyze.length,
          analysisMethod: 'chunked',
          model: 'gpt-4o-mini'
        }
      })
    }

  } catch (error) {
    console.error('Summarize error:', error)
    return NextResponse.json({
      error: 'Failed to summarize chat',
      details: error instanceof Error ? error.message : 'Unknown error'
    }, { status: 500 })
  }
}
